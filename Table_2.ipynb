{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9f252e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "#100 bootstrap resampling parameter\n",
    "n_samples = 100\n",
    "    \n",
    "#process classifications to convert to binary labels\n",
    "def map_to_number(x):\n",
    "    if x=='a' or x=='use' or x=='used':\n",
    "        return 0\n",
    "    elif x=='b' or x=='mention' or x=='mentioned' or x=='m':\n",
    "        return 1\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "#helpers for classifying metrics \n",
    "def false_positive(true, predicted):\n",
    "    CM = confusion_matrix(true, predicted)\n",
    "\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0]\n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "\n",
    "    return(100*FP/(FP+TN))\n",
    "    \n",
    "def false_negative(true, predicted):\n",
    "    CM = confusion_matrix(true, predicted)\n",
    "\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0]\n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "\n",
    "    return(100*FN/(FN + TP))\n",
    "    \n",
    "\n",
    "def get_metrics(true_o, predicted_o):\n",
    "    rs = []\n",
    "    ps = []\n",
    "    acs = []\n",
    "    for i in range(n_samples):\n",
    "        t = df.sample(len(df), replace = True)\n",
    "        true = t.dropna(subset = [model])['type'].apply(lambda x: 1 if x =='disapproving' else 0)\n",
    "        predicted = t.dropna(subset = [model])[model]\n",
    "        rs.append(false_positive(true, predicted))\n",
    "        ps.append(false_negative(true, predicted))\n",
    "        acs.append((1 - accuracy_score(true, predicted))*100)\n",
    "        \n",
    "        \n",
    "    return false_negative(true_o, predicted_o), false_positive(true_o, predicted_o), 100*(1 - accuracy_score(true_o, predicted_o)),\\\n",
    "    np.nanpercentile(ps,97.5), np.nanpercentile(rs,97.5), np.nanpercentile(acs,97.5),\\\n",
    "    np.nanpercentile(ps,2.5), np.nanpercentile(rs,2.5), np.nanpercentile(acs,2.5),\\\n",
    "    np.nanpercentile(ps,50), np.nanpercentile(rs,50), np.nanpercentile(acs,50)\n",
    "\n",
    "#tested models list helpers\n",
    "models = ['label_gpt3.5',\n",
    "          'label_gpt4',\n",
    "          'label_gpt3.5instruct']\n",
    "\n",
    "#load the dataset with classifications\n",
    "table = pd.read_csv('data/task1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7073400",
   "metadata": {},
   "source": [
    "### Calculate metrics for hate subtask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b57e261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_entries = []\n",
    "\n",
    "#select hate task samples\n",
    "df = table.loc[table.subtask=='hate'].reset_index()\n",
    "\n",
    "#clean and process classifications (removing trailing characters and mapping to numbers)\n",
    "df['label_gpt4'] = df['label_gpt4'].apply(lambda x: str(x).lower().strip('\"').strip('.').strip(' '))\n",
    "df['label_gpt3.5'] = df['label_gpt3.5'].apply(lambda x: str(x).lower().strip('\"').strip('.').strip(' '))\n",
    "df['label_gpt3.5instruct'] = df['label_gpt3.5instruct'].apply(lambda x: str(x).lower().strip('\"').strip('.').strip(' '))\n",
    "\n",
    "df['label_gpt4'] = df['label_gpt4'].apply(map_to_number)\n",
    "df['label_gpt3.5'] = df['label_gpt3.5'].apply(map_to_number)\n",
    "df['label_gpt3.5instruct'] = df['label_gpt3.5instruct'].apply(map_to_number)\n",
    "\n",
    "#calculate the perfomance for each model\n",
    "for cnt,model in enumerate(models):\n",
    "    entry = {}\n",
    "    entry['task'] = 'hate speech'\n",
    "    entry['model'] = model\n",
    "    \n",
    "    true = df.dropna(subset = [model])['type'].apply(lambda x: 1 if x =='disapproving' else 0)\n",
    "    predicted = df.dropna(subset = [model])[model]\n",
    "    \n",
    "    metrics = get_metrics(true, predicted)\n",
    "    entry['False positive rate'] = metrics[0]\n",
    "    entry['False negative rate'] = metrics[1]\n",
    "    entry['Average error rate'] = metrics[2]\n",
    "    \n",
    "    entry['False positive rate upper'] = metrics[3]\n",
    "    entry['False negative rate upper'] = metrics[4]\n",
    "    entry['Average error rate upper'] = metrics[5]\n",
    "    \n",
    "    entry['False positive rate lower'] = metrics[6]\n",
    "    entry['False negative rate lower'] = metrics[7]\n",
    "    entry['Average error rate lower'] = metrics[8]\n",
    "    \n",
    "    entry['False positive rate median'] = metrics[9]\n",
    "    entry['False negative rate median'] = metrics[10]\n",
    "    entry['Average error rate median'] = metrics[11]\n",
    "    \n",
    "    list_entries.append(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083bb14e",
   "metadata": {},
   "source": [
    "### Calculate metrics for misinformation subtask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41a50f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select misinformation task samples\n",
    "df = table.loc[table.subtask=='misinfo'].reset_index()\n",
    "\n",
    "#clean and process classifications (removing trailing characters and mapping to numbers)\n",
    "df['label_gpt4'] = df['label_gpt4'].apply(lambda x: str(x).lower().strip('\"').strip('.').strip(' '))\n",
    "df['label_gpt3.5'] = df['label_gpt3.5'].apply(lambda x: str(x).lower().strip('\"').strip('.').strip(' '))\n",
    "df['label_gpt3.5instruct'] = df['label_gpt3.5instruct'].apply(lambda x: str(x).lower().strip('\"').strip('.').strip(' '))\n",
    "\n",
    "df['label_gpt4'] = df['label_gpt4'].apply(map_to_number)\n",
    "df['label_gpt3.5'] = df['label_gpt3.5'].apply(map_to_number)\n",
    "df['label_gpt3.5instruct'] = df['label_gpt3.5instruct'].apply(map_to_number)\n",
    "\n",
    "#calculate the perfomance for each model\n",
    "for cnt,model in enumerate(models):\n",
    "    entry = {}\n",
    "    entry['task'] = 'misinformation'\n",
    "    entry['model'] = model\n",
    "    \n",
    "    true = df.dropna(subset = [model])['type'].apply(lambda x: 1 if x =='disapproving' else 0)\n",
    "    predicted = df.dropna(subset = [model])[model]\n",
    "    \n",
    "    metrics = get_metrics(true, predicted)\n",
    "    entry['False positive rate'] = metrics[0]\n",
    "    entry['False negative rate'] = metrics[1]\n",
    "    entry['Average error rate'] = metrics[2]\n",
    "    \n",
    "    entry['False positive rate upper'] = metrics[3]\n",
    "    entry['False negative rate upper'] = metrics[4]\n",
    "    entry['Average error rate upper'] = metrics[5]\n",
    "    \n",
    "    entry['False positive rate lower'] = metrics[6]\n",
    "    entry['False negative rate lower'] = metrics[7]\n",
    "    entry['Average error rate lower'] = metrics[8]\n",
    "    \n",
    "    entry['False positive rate median'] = metrics[9]\n",
    "    entry['False negative rate median'] = metrics[10]\n",
    "    entry['Average error rate median'] = metrics[11]\n",
    "     \n",
    "    list_entries.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5603588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=pd.DataFrame(list_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4105970d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>model</th>\n",
       "      <th>False positive rate</th>\n",
       "      <th>False positive rate error</th>\n",
       "      <th>False negative rate</th>\n",
       "      <th>False negative rate error</th>\n",
       "      <th>Average error rate</th>\n",
       "      <th>Average error rate error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hate speech</td>\n",
       "      <td>label_gpt3.5instruct</td>\n",
       "      <td>17.98</td>\n",
       "      <td>8.31</td>\n",
       "      <td>14.77</td>\n",
       "      <td>6.83</td>\n",
       "      <td>16.38</td>\n",
       "      <td>5.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hate speech</td>\n",
       "      <td>label_gpt3.5</td>\n",
       "      <td>6.82</td>\n",
       "      <td>5.59</td>\n",
       "      <td>20.00</td>\n",
       "      <td>7.93</td>\n",
       "      <td>13.48</td>\n",
       "      <td>4.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hate speech</td>\n",
       "      <td>label_gpt4</td>\n",
       "      <td>20.00</td>\n",
       "      <td>7.85</td>\n",
       "      <td>4.44</td>\n",
       "      <td>3.83</td>\n",
       "      <td>12.22</td>\n",
       "      <td>4.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>misinformation</td>\n",
       "      <td>label_gpt3.5instruct</td>\n",
       "      <td>34.38</td>\n",
       "      <td>2.77</td>\n",
       "      <td>40.08</td>\n",
       "      <td>3.02</td>\n",
       "      <td>37.22</td>\n",
       "      <td>2.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>misinformation</td>\n",
       "      <td>label_gpt3.5</td>\n",
       "      <td>8.05</td>\n",
       "      <td>1.79</td>\n",
       "      <td>49.76</td>\n",
       "      <td>4.01</td>\n",
       "      <td>28.93</td>\n",
       "      <td>2.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>misinformation</td>\n",
       "      <td>label_gpt4</td>\n",
       "      <td>23.44</td>\n",
       "      <td>2.76</td>\n",
       "      <td>3.89</td>\n",
       "      <td>1.08</td>\n",
       "      <td>13.64</td>\n",
       "      <td>1.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             task                 model  False positive rate  \\\n",
       "2     hate speech  label_gpt3.5instruct                17.98   \n",
       "0     hate speech          label_gpt3.5                 6.82   \n",
       "1     hate speech            label_gpt4                20.00   \n",
       "5  misinformation  label_gpt3.5instruct                34.38   \n",
       "3  misinformation          label_gpt3.5                 8.05   \n",
       "4  misinformation            label_gpt4                23.44   \n",
       "\n",
       "   False positive rate error  False negative rate  False negative rate error  \\\n",
       "2                       8.31                14.77                       6.83   \n",
       "0                       5.59                20.00                       7.93   \n",
       "1                       7.85                 4.44                       3.83   \n",
       "5                       2.77                40.08                       3.02   \n",
       "3                       1.79                49.76                       4.01   \n",
       "4                       2.76                 3.89                       1.08   \n",
       "\n",
       "   Average error rate  Average error rate error  \n",
       "2               16.38                      5.30  \n",
       "0               13.48                      4.54  \n",
       "1               12.22                      4.72  \n",
       "5               37.22                      2.27  \n",
       "3               28.93                      2.28  \n",
       "4               13.64                      1.64  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate the confidence interval (upper error - lower error)\n",
    "res['Average error rate error'] = (res['Average error rate upper'] - res['Average error rate lower'])/2\n",
    "res['False positive rate error'] = (res['False positive rate upper'] - res['False positive rate lower'])/2\n",
    "res['False negative rate error'] = (res['False negative rate upper'] - res['False negative rate lower'])/2\n",
    "\n",
    "#print the table\n",
    "res[['task','model','False positive rate','False positive rate error',\n",
    "     'False negative rate','False negative rate error',\n",
    "     'Average error rate','Average error rate error']].\\\n",
    "    sort_values(by = 'Average error rate',ascending = False).sort_values(by = 'task',ascending = True).round(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
